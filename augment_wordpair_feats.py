import re
import time
import io
import sys
import argparse
from collections import defaultdict

# parse/validate arguments
argParser = argparse.ArgumentParser()
argParser.add_argument("-sb", "--src_brown_filename")
argParser.add_argument("-tb", "--tgt_brown_filename")
argParser.add_argument("-i", "--input_filename") # this is the word alignment wordpair features generated by cdec
argParser.add_argument("-o", "--output_filename")
args = argParser.parse_args()

input_wordpair_file = io.open(args.input_filename, encoding='utf8', mode='r')
output_wordpair_file = io.open(args.output_filename, encoding='utf8', mode='w')

def read_brown_clusters(brown_filename):
  brown_file = io.open(brown_filename, encoding='utf8', mode='r')
  type_to_cluster, type_to_freq = {}, {}
  freqs = []
  for line in brown_file:
    if len(line.strip()) == 0: continue
    if line.startswith('#'): continue
    bitstring, word_type, freq = line.strip().split('\t')
    type_to_cluster[word_type] = bitstring
    type_to_freq[word_type] = int(freq)
    freqs.append(int(freq))
  brown_file.close()
  freqs.sort()
  return type_to_cluster, type_to_freq, freqs

# freq_score \in \{0 \ldots max_score}
def compute_freq_score(word, word_freqs, sorted_freqs, max_score=4):
  fold_length = len(sorted_freqs) / (max_score-1);
  # special case: return score  = 1 if the word is out of vocab (with respect to word_freqs)
  if word not in word_freqs: return 0
  # special case: return score = max_score for the most frequent 100 words
  if word_freqs[word] > sorted_freqs[-100]: return max_score
  for fold in range(1, max_score):
    if word_freqs[word] < sorted_freqs[fold * fold_length]:
      return fold
  # this should only happen in peculiar cases (e.g. max_score > 100)
  return max_score

# read src/tgt clusters
src_brown_clusters, src_word_freqs, src_sorted_freqs = read_brown_clusters(args.src_brown_filename)
tgt_brown_clusters, tgt_word_freqs, tgt_sorted_freqs = read_brown_clusters(args.tgt_brown_filename)

print src_brown_clusters[u'.'] 
print tgt_brown_clusters[u'.']

# read/write word pairs
for line in input_wordpair_file:
  if len(line.strip()) == 0: continue
  src_word, tgt_word, features = line.strip().split(' ||| ')
  # first, determine the full brown cluster bitstring of src_word and tgt_word
  src_cluster = src_brown_clusters[src_word] if src_word in src_brown_clusters else u'?'
  tgt_cluster = tgt_brown_clusters[tgt_word] if tgt_word in tgt_brown_clusters else u'?'
  # then, determine how frequent src_word and tgt_word are
  src_freq_score = compute_freq_score(src_word, src_word_freqs, src_sorted_freqs)
  tgt_freq_score = compute_freq_score(tgt_word, tgt_word_freqs, tgt_sorted_freqs)
  # generate a bunch of binary features
  new_features = set()
  # the full src/tgt clusters
  new_features.add(u'Brown{}:{}=1'.format(src_cluster, tgt_cluster))
  # prefixes of the src/tgt clusters
  new_features.add(u'Brown{}:{}=1'.format(src_cluster[0:4], tgt_cluster[0:4]))
  new_features.add(u'Brown{}:{}=1'.format(src_cluster[0:8], tgt_cluster[0:8]))
  new_features.add(u'Brown{}:{}=1'.format(src_cluster[0:12], tgt_cluster[0:12]))
  # frequency for src word and tgt word
  new_features.add(u'Freq{}:{}=1'.format(src_freq_score, tgt_freq_score))
  new_features.add(u'FreqDiff{}=1'.format(abs(src_freq_score - tgt_freq_score)))
  # write the new features to the end of the line
  line = u'{} ||| {} ||| {} {}\n'.format( src_word, tgt_word, features, u' '.join(new_features) )
  output_wordpair_file.write(line)
  
input_wordpair_file.close()
output_wordpair_file.close()
