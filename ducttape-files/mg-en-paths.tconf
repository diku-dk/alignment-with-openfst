global {

    train_corpus="/usr3/home/wammar/subset.en-fi.tok.lc.max5.uniq"
    # train_corpus="/usr0/home/wammar/alignment-with-openfst/toy-data/love.en-ar"
    tune_corpus="/usr3/home/wammar/subset.en-fi.tok.lc.max5.uniq"
    # tune_corpus="/usr0/home/wammar/alignment-with-openfst/toy-data/love.en-ar"
    test_corpus="/usr3/home/wammar/subset.en-fi.tok.lc.max5.uniq"
    # test_corpus="/usr0/home/wammar/alignment-with-openfst/toy-data/love.en-ar"
    src_brown_clusters="/usr3/home/wammar/brown-clusters/malagasy-c80-p1.out/paths"
    tgt_brown_clusters="/usr1/home/wammar/parallel/english/news-commentary10.cz-en.en.tok.brown80"

    # only specify when you want to reuse a previously built LM 
    language_model=""
    # only use these two parameters if you want to build a language model. if you have a language model already built, specify "language_model=" instead
    lm_order=4
    lm_data="/usr1/home/wammar/monolingual/lm-for-wmt11-czen/text"
    
    # tool paths
    cdec_dir="/home/wammar/cdec/"
    multeval="/home/wammar/git/multeval/multeval.sh"
    giza_bin="/opt/tools/mgizapp-0.7.2/bin"
    moses_train_script="/home/wammar/git/mosesdecoder/scripts/training/train-model.perl"
    mkcls_bin="/mal0/tools/mosesdecoder/bin/mkcls"
    wammar_utils_dir="/home/wammar/wammar-utils"
    # alignment_with_openfst_dir="/home/wammar/stochastic-crf-autoencoder/alignment-with-openfst/"
    alignment_with_openfst_dir="/usr3/home/fanyang1/alignment-with-openfst/"
    kenlm_dir="/home/wammar/git/kenlm"

    # aer
    conv_pharaoh_script="/home/wammar/alignment-with-openfst/data/hansards/conv-pharaoh.pl"
    aer_eval_script=""
    gold_alignment=""

    # other aligner outputs
    fwd_giza_alignments=""
    bwd_giza_alignments=""
    sym_giza_alignments=""
    fwd_fast_alignments=""
    bwd_fast_alignments=""
    sym_fast_alignments=""
}
